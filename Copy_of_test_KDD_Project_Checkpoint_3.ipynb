{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmaganji/KDD-Project/blob/main/Copy_of_test_KDD_Project_Checkpoint_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6x2Nw0d3RsTm"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "import scipy\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jxawPfaBR9dJ"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8J8bqYM9SGrl"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(io.BytesIO(uploaded['rideshare_kaggle.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('rideshare_kaggle.csv')"
      ],
      "metadata": {
        "id": "6kWdtiBjjM3d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYv9EYHPYoxm"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB3qUoeWY90A"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk2yhGbJbF46"
      },
      "outputs": [],
      "source": [
        "#Total null values in every column\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD7jCzB9SF1q"
      },
      "outputs": [],
      "source": [
        "# Removing rows with missing values\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hfR5sHyqCa9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBevjYYMSbxl"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdESpd2KbU2-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_FwmZHEbaIj"
      },
      "outputs": [],
      "source": [
        "#correlation matrix\n",
        "\n",
        "corrmat = df.corr()\n",
        "f, ax = plt.subplots(figsize=(20, 20))\n",
        "sns.heatmap(corrmat, square=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIdbeZNcc7Oh"
      },
      "outputs": [],
      "source": [
        "#from the correlation heat map we can remove all the unnecessary variable that has no correlations.\n",
        "\n",
        "df.drop(df.columns[[0,1,5,6,10,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56]], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs8Pf_vVf0Xl"
      },
      "outputs": [],
      "source": [
        "data = df\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyzCqR9L3oY-"
      },
      "outputs": [],
      "source": [
        "# Apply factorize encoding to the 'source' and 'destination' features\n",
        "data['source_encoded'] = pd.factorize(data['source'])[0]\n",
        "data['destination_encoded'] = pd.factorize(data['destination'])[0]\n",
        "\n",
        "# Drop the original 'source' and 'destination' features from the data\n",
        "data.drop(['source', 'destination'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw47dZlEnnXn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jin_SGXqA4u3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSam9khTi68R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxr5P2_gUsH8"
      },
      "source": [
        "This indicates that a significant portion of the taxis in New York City fall within the price range of \\$5 to $20. There is a considerable demand for cab rides at these prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Ur8oSPAnYL"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of Price\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
        "ax = sns.histplot(data['price'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Price\");\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Number of Trips')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQT2rmzaWHqC"
      },
      "source": [
        "This suggests that the majority of taxis in New York City tend to travel distances of up to 3.5 to 4 miles from the city center. However, there are relatively fewer trips made for distances exceeding 4 miles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRKp66m5Ap9B"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of Distance\n",
        "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
        "ax = sns.histplot(data['distance'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Distance\");\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Number of Trips')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAOX9Y9AXLtC"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsUyGC5XndL"
      },
      "source": [
        "Analyzing both diagrams, we can deduce that the longest cab journeys are frequently undertaken to and from Boston University, while the shortest trips typically originate from Haymarket Square."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6rwIz0i_87w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot the first diagram\n",
        "sns.barplot(x=\"source_encoded\", y=\"distance\", data=df, color=\"darkblue\", ax=axs[0])\n",
        "axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=90)\n",
        "axs[0].set_title(\"Source vs Distance\")\n",
        "\n",
        "# Plot the second diagram\n",
        "sns.barplot(x=\"destination_encoded\", y=\"distance\", data=df, color=\"darkblue\", ax=axs[1])\n",
        "axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=90)\n",
        "axs[1].set_title(\"Destination vs Distance\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yP5WKX3_9nO"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28Ts_xmOnpt-"
      },
      "outputs": [],
      "source": [
        "# we have ploted the dataset with respect to distance and price using seaborn scatterplot.\n",
        "fig, ax = plt.subplots(figsize=(15, 6), dpi=100)\n",
        "ax = sns.scatterplot(x=\"distance\",\n",
        "                    y=\"price\",\n",
        "                    data = data,\n",
        "                    hue = \"cab_type\",\n",
        "                    size = 10,\n",
        "                    legend = 'brief')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM9G4EhtqzHT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKgz9Yd_3sV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0scggsBRFuYr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz26JL9rJcrN"
      },
      "outputs": [],
      "source": [
        "# Aggregate the data by hour and calculate the count of cab bookings\n",
        "grouped_df = data.groupby('hour').size().reset_index(name='cab_bookings')\n",
        "\n",
        "# Create the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='hour', y='cab_bookings', data=grouped_df)\n",
        "plt.xticks(range(24))\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Cab Bookings')\n",
        "plt.title('Cab Bookings over Hours')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xrLqQGsbwNd"
      },
      "source": [
        "  In below visualization, We can observe that there are constant number of booking regardless of temperature, but we can observe, there is rise in bookings when temperature is 35-45 degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONLaeAk7YVx0"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each temperature value\n",
        "temp_counts = data['temperature'].value_counts().reset_index()\n",
        "temp_counts.columns = ['temperature', 'count']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(temp_counts['temperature'], temp_counts['count'])\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Number of Bookings')\n",
        "plt.title('Temperature vs. Number of Bookings')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UljOKirY1Az"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Encode categorical columns using label encoding\n",
        "data['cab_type_encoded'] = encoder.fit_transform(data['cab_type'])\n",
        "data['name_encoded'] = encoder.fit_transform(data['name'])\n",
        "data['icon_encoded'] = encoder.fit_transform(data['icon'])\n",
        "\n",
        "data.drop(['cab_type', 'name', 'icon'], axis=1, inplace=True)\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuY2_Z46j6Uy"
      },
      "source": [
        "*Feature Scaling*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS-iJ1vDecMG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select the numerical features for scaling\n",
        "numerical_features = ['hour', 'day', 'month', 'distance', 'surge_multiplier', 'temperature']\n",
        "\n",
        "# Create a new DataFrame with only the numerical features\n",
        "numerical_data = data[numerical_features]\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the numerical data\n",
        "scaler.fit(numerical_data)\n",
        "\n",
        "# Transform the numerical data\n",
        "scaled_data = scaler.transform(numerical_data)\n",
        "\n",
        "# Replace the original numerical columns with the scaled values\n",
        "data[numerical_features] = scaled_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ypw9Syn7j---"
      },
      "outputs": [],
      "source": [
        "# Select the features and target variable\n",
        "features = data.drop('price', axis=1)\n",
        "target = data['price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5MugtnJkNCJ"
      },
      "outputs": [],
      "source": [
        "#Splitting the test and train dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yn9pwjtXID4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data.to_csv('data.csv', index=False)\n",
        "file_path = os.getcwd() + 'data.csv'\n",
        "print(file_path)\n",
        "# files.download('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nWd8kXW6G1y"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_28qkLn5TVK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmJW_hY1zAmI"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVtJDVIXzQrQ"
      },
      "outputs": [],
      "source": [
        "#Splitting the test and train dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(features, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLJFmA5RjV36"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the training data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "linear = LinearRegression()\n",
        "linear.fit(X_train1, y_train1)\n",
        "\n",
        "# Making predictions on the test data\n",
        "y_pred1 = linear.predict(X_test1)\n",
        "\n",
        "# Evaluating the model\n",
        "\n",
        "print(f'Linear Regression:')\n",
        "mse_linear = mean_squared_error(y_test1, y_pred1)\n",
        "print(f'MSE = {mse_linear}')\n",
        "\n",
        "rmse_linear = np.sqrt(mean_squared_error(y_test1, y_pred1))\n",
        "print(f'RMSE = {rmse_linear}')\n",
        "\n",
        "r2_linear = linear.score(X_test1, y_test1)\n",
        "print(f'R2 = {r2_linear}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm0JhdRwvEUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhINvy4rzEYv"
      },
      "source": [
        "## Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnRrRW8LzS55"
      },
      "outputs": [],
      "source": [
        "#Splitting the test and train dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(features, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06bR1bVuvFDQ"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the training data\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train2, y_train2)\n",
        "\n",
        "# Making predictions on the test data\n",
        "y_pred2 = lasso.predict(X_test2)\n",
        "\n",
        "# Evaluating the model\n",
        "\n",
        "print(f'Lasso Model:')\n",
        "mse_lasso = mean_squared_error(y_test2, y_pred2)\n",
        "print(f'MSE = {mse_lasso}')\n",
        "\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test2, y_pred2))\n",
        "print(f'RMSE = {rmse_lasso}')\n",
        "\n",
        "r2_lasso = lasso.score(X_test2, y_test2)\n",
        "print(f'R2 = {r2_lasso}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwoPFKYZz9GL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crngb8Ef0IdK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4-YPVvi6YC-"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5skVunL7Ptm"
      },
      "outputs": [],
      "source": [
        "#Splitting the test and train dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(features, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtCOmk917Uyo"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the training data\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train3, y_train3)\n",
        "\n",
        "# Making predictions on the test data\n",
        "y_pred3 = regressor.predict(X_test3)\n",
        "\n",
        "# Evaluating the model\n",
        "\n",
        "print(f'Decision Tree Regressor')\n",
        "mse_decision_tree = mean_squared_error(y_test3, y_pred3)\n",
        "print(f'MSE = {mse_decision_tree}')\n",
        "\n",
        "rmse_decision_tree = np.sqrt(mean_squared_error(y_test3, y_pred3))\n",
        "print(f'RMSE = {rmse_decision_tree}')\n",
        "\n",
        "r2_decision_tree = regressor.score(X_test3, y_test3)\n",
        "print(f'R2 = {r2_decision_tree}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmnAO8IO87YU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yu_6F2zzEu6"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLyO4DEzLCL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [10, 20, 30, 40],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator = DecisionTreeRegressor(), param_grid = param_grid,\n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "grid_search.fit(X_train3, y_train3)\n",
        "grid_search.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7_1JlzBzQim"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}